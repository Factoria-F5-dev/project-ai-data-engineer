# ğŸš€ PROYECTO DATA ENGINEER: Proceso ETL

![Data Engineer](https://github.com/user-attachments/assets/724b03b7-74f1-4a28-b0da-65484e58e827)

## ğŸ“œ Briefing: PROYECTO DATA ENGINEER

### ğŸ” Planteamiento  

Bienvenidos al equipo de ingenieros de datos freelance de DataTech Solutions.
Han sido seleccionados para abordar un proyecto crucial para nuestro cliente, una empresa
lÃ­der en recursos humanos llamada "HR Pro".
El objetivo del proyecto es diseÃ±ar e implementar un sistema de gestiÃ³n de datos eficiente
para HR Pro, que les permita organizar y analizar grandes volÃºmenes de datos procedentes
de diversas fuentes, como solicitudes de empleo, registros de nÃ³mina, encuestas de
empleados, entre otros. Cada uno de ustedes aporta habilidades Ãºnicas en extracciÃ³n,
transformaciÃ³n y carga de datos (ETL), asÃ­ como experiencia en el uso de tecnologÃ­as como
Apache Kafka, MongoDB y bases de datos SQL.
DeberÃ¡n diseÃ±ar un proceso ETL robusto para integrar datos en un sistema unificado,
organizar y almacenar datos de manera eficiente tanto en una base de datos MongoDB
como en un almacÃ©n de datos SQL, implementar el sistema en un entorno Dockerizado
para garantizar la escalabilidad y la portabilidad, y trabajar con una amplia variedad de
datos, desde informaciÃ³n personal hasta registros financieros y mÃ©tricas de rendimiento.
Se espera que entreguen un sistema funcional y bien documentado que permita a HR Pro
gestionar y analizar eficientemente sus datos de recursos humanos.

#### DISCLAIMER
Los datos no son reales, sino que se van generando de manera aleatoria y enviandose a un
servidor de Apache Kafka. Todo el cÃ³digo para lanzar tanto el servidor Kafka como el
generador de datos se encuentra en este repositorio de GitHub, el cuÃ¡l tambiÃ©n estÃ¡
documentado con instrucciones sobre su uso:
[proyecto](https://github.com/Factoria-F5-dev/data-engineering-educational-project)
Es **ABSOLUTAMENTE FUNDAMENTAL** que **NO** se acceda a leer el cÃ³digo que genera
esos datos.
Hacerlo supondrÃ­a conocer los detalles de por quÃ© los datos son cÃ³mo son y eso estarÃ­a
muy alejado de lo que serÃ­a un caso real, reduciendo el aspecto pedagÃ³gico del proyecto.
---

## ğŸ¯ Objetivos del Proyecto  

* **Implementar proceso ETL.**  
* **Preprocesar los datos.**  
* **Trabajar con colas de mensajes.**  
* **Implementar bases de datos SQL/NoSQL.**   

---

## ğŸ“¦ Condiciones de Entrega  

Para la fecha de entrega, los equipos deberÃ¡n presentar:  

âœ… **Repositorio en GitHub** con el cÃ³digo fuente documentado.

âœ… **Un programa dockerizado** que se conecte al servidor de Kafka, procese los datos que lee y los guarde de manera ordenada en una base de datos MongoDB y una SQL a elegir

âœ… **Demo en vivo** mostrando el funcionamiento de la aplicacion.

âœ… **PresentaciÃ³n tÃ©cnica**, explicando los objetivos, desarrollo y tecnologÃ­as utilizadas.

âœ… **Tablero Kanban** con la gestiÃ³n del proyecto (Trello, Jira, etc.).  

---

## âš™ï¸ TecnologÃ­as Recomendadas  

- **Control de versiones:** Git / GitHub  
- **Entorno de ejecuciÃ³n:** Docker  
- **Lenguaje principal:** Python  
- **LibrerÃ­as Ãºtiles:** Kafka, Pandas
- **Bases de Datos:** MongoDB, SQL
- **GestiÃ³n del proyecto:** Trello, Jira, Github  

---

## ğŸ† Niveles de Entrega  

### ğŸŸ¢ **Nivel Esencial:**  
âœ… Configurar correctamente el consumer de Kafka para consumir los miles de mensajes por segundo del servidor de kafka en tiempo real.

âœ… Persistir los mensajes de kafka en una base de datos documental (por ejemplo MongoDB).

âœ… Procesar los datos y agrupar los distintos tipos de datos de cada persona (Personal data, Location, Professional data, Bank Data y Net Data) en un Ãºnico registro.

âœ… Persistir los datos procesados y agrupados en una base de datos relacional (MySQL, Postgresâ€¦).

âœ… Repositorio Git con ramas bien organizadas y commits limpios y descriptivos.

âœ… DocumentaciÃ³n del cÃ³digo y un README en GitHub.  

### ğŸŸ¡ **Nivel Medio:**  
âœ… Incluir un sistema de logs.

âœ… Incluir tests unitarios.

âœ… Dockerizar la aplicaciÃ³n utilizando docker y docker compose.


### ğŸŸ  **Nivel Avanzado:**  
âœ… Utilizar una base de datos en cachÃ© (por ejemplo redis) como base de datos intermedia antes de almacenar los datos en otro tipo de sistema de almacenamiento, para mejorar la captura de datos en tiempo real.

âœ… Monitorizar el funcionamiento de la aplicaciÃ³n (cuÃ¡ntos mensajes se consumen, a quÃ© velocidad, cuanto tardan en procesarse, cuanto tardan en persistirse, rendimiento general de la aplicaciÃ³n, etc). Se pueden utilizar herramientas como Prometheus.

âœ… Crear una API para conectarse a la base de datos relacional y poder hacer consultas sobre la informaciÃ³n final procesada disponible en la base de datos SQL.

### ğŸ”´ **Nivel Experto:**  
âœ… Automatizar la carga de datos para que la informaciÃ³n de las bases de datos se vaya actualizando de forma continua mientras el servidor de kafka se mantiene ejecutÃ¡ndose y enviando mensajes.

âœ… Crear una frontend sencillo (por ejemplo: streamlite, gradio) desde el que poder consultar los datos de los clientes.

---

## ğŸ“Š EvaluaciÃ³n  

Se considerarÃ¡n los siguientes criterios:  

âœ… Procesar y almacenar datos de eventos en tiempo real
 

MÃ¡s detalles en: [roadmap-mad-ai-p4.coderf5.es](https://roadmap-mad-ai-p4.coderf5.es/)  

 

